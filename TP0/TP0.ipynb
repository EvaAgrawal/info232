{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"logo.png\", width=150, ALIGN=\"left\">\n",
    "<center>\n",
    "<h1>Mini Projets 2019-2020 (Info 232)</h1>\n",
    "Isabelle Guyon <br>\n",
    "info232@chalearn.org <br>\n",
    "</center>\n",
    "<span style=\"color:red\"> <h1> 0 . Intro&Tools </h1> </span>\n",
    "    \n",
    "<p> Learn about the tools we will be using in this class: Python (<a href=\"https://www.anaconda.com/distribution/?gclid=CjwKCAiApOvwBRBUEiwAcZGdGJfRGe75diel7fHcnmosDCyneXkUuFd3CEP4HBTlAl4m30afyOJW1RoC6DEQAvD_BwE\">Anaconda distribution for Python 3.7</a>), <a href=\"https://www.spyder-ide.org/\">the Spyder editor</a> coming with Anaconda and <a href=\"https://jupyter-notebook.readthedocs.io/en/stable/\">Jupyter notebooks</a> allowing you to easily develop Python code. Get quickly aquainted with three Python libraries we will be using all the time: <a href=\"https://numpy.org/\">Numpy</a>, <a href=\"https://pandas.pydata.org/\">Pandas</a>, and <a href=\"https://scikit-learn.org/stable/\">Scikit-learn</a> (also called sklearn). Learn about Git and <a href=\"https://github.com/\">Github</a> to work collaboratively with your team and keep your code under revision control. Learn to submit your homework with <a href=\"https://chagrade.lri.fr/\">ChaGrade</a> and check out your challenge on <a href=\"https://codalab.lri.fr/my/\">Codalab</a>.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Instructions </h2>\n",
    "    <p>\n",
    "<span style=\"color:blue\">        TP means \"Travaux Pratiques\" i.e. exercise. Each week you will start your TP in class, then finish it at home, if necessary, and submit it AT THE LATEST on <b>Saturday</b> (strict deadline). The teaching assistants will give you a <b>provisional grade</b> that you can still <b>improve IF you show up at the Python support class on Wednesday</b>.</span>\n",
    "<p> \n",
    "        This TP gives you 5 points if you answer well at least 5 questions. However we encourage you answer all questions: they are meant to give you ideas you can use later in your projects.<br>\n",
    "    <ul>\n",
    "        <li> To create a new cell, go to the menu \"Insert\".</li>\n",
    "        <li> To transform a cell to a \"comment\", go to Cell + Cell Type + Markdown. </li>\n",
    "        <li> To execute a cell: SHIFT+RETURN </li>\n",
    "        <li> <b> The cells must be executed in order.</b> </li>\n",
    "    </ul>\n",
    "    </p>     \n",
    "<span style=\"color:red\"> <b>Save your notebook often with menu File + Save and Checkpoint.</b>\n",
    "<br> <b>Before you push your homework to your GitHub repo, use  Kernel + Restart and Run all.</b>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 0: \"Markdown\" cells\n",
    "\n",
    "Markdown cells are very handy to insert text or images in your code.\n",
    "<br>Create a <b>new cell</b> of type Markdown below this one.\n",
    "Copy the paragraph on cross-validation that you will find in Wikipedia. Try to color the cell in GREEN! \n",
    "<br> Tip: get inspired by the code of the cell above this one by double-clicking on it. \n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background :green'>\n",
    "Cross-validation, sometimes called rotation estimation[1][2][3] or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[4][5] The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[6] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: \"Code\" cells\n",
    "\n",
    "Execute the cell below after replacing the answer by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#00FF00\">CORRECT<br>:-)</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Block of code needed to run the rest of the notebook smoothly:\n",
    "# Make is possible to include graphics in notebook\n",
    "%matplotlib inline\n",
    "# Reload py files after edits were made automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### Add path to the sample code so the notebook finds it:\n",
    "code_dir = 'code/'                        \n",
    "from sys import path; path.append(code_dir)\n",
    "\n",
    "# Import code that checks your answers\n",
    "from checker import check \n",
    "# Disable some warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "### Now your question:\n",
    "question = 1\n",
    "answer = 1  # Replace by 1\n",
    "score = 0\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Data files\n",
    "\n",
    "In your projects, you will have to analyze data in <a href=\"https://github.com/codalab/chalab/wiki/Help:-Wizard-%E2%80%90-Challenge-%E2%80%90-Data\">AutoML format</a>. The data are usually split into a <b>training set</b> to develop your models (called \"train\") and <b>two test sets</b> (a validation set called \"valid\" used for testing your models during the \"development phase\" and a true \"test\" set for the final evaluation of your greatest model when the project is over). This way the \"test\" set is not compromised by having been used to select your best model.\n",
    "\n",
    "Using any means available to you, inspect the \"iris\" files found in the directory \"info232/TP0/data/\" and fill out the answers.\n",
    "<br> Idea 1: inspect the files with an editor. \n",
    "<br> Idea 2: remplace the <a href=\"http://cheatsheetworld.com/programming/unix-linux-cheat-sheet/\"> Unix command</a> \"ls\" by another one like \"wc\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "data_dir = r'C:\\Users\\pankaj NIC\\GitHub\\info232\\TP0\\data'             \n",
    "data_name = 'iris_feat.name'\n",
    "# The character ! lets you \"escape\" to the Unix shell\n",
    "!wc $data_dir/*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#00FF00\">CORRECT<br>:-)</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_number = 4            # put correct value\n",
    "training_sample_number = 35   # put correct value\n",
    "validation_sample_number = 35 # put correct value\n",
    "test_sample_number = 35       # put correct value\n",
    "question = 2\n",
    "answer = feature_number*(training_sample_number+validation_sample_number+test_sample_number)\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Code files\n",
    "In your project, you will be provided with some sample code to help you get started. \n",
    "<br>\n",
    "Using <a href=\"https://www.spyder-ide.org/\">the Spyder editor</a>, inspect the code in the \"code/\" directory. To do so, open a command window and navigate to the directory \"info232/TP0/code/\" the, at the prompt, run the command:\n",
    "```\n",
    "spyder data_io.py &\n",
    "```\n",
    "then replace the variable values in the cell below by meaningful answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_numpy_as = \"np\"\n",
    "import_pandas_as = \"pd\"\n",
    "name_of_first_argument_of_read_as_df = \"basename\"\n",
    "default_value_of_second_argument_of_read_as_df=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background:#00FF00\">CORRECT<br>:-)</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mystring = import_numpy_as+import_pandas_as+name_of_first_argument_of_read_as_df+default_value_of_second_argument_of_read_as_df\n",
    "question = 3\n",
    "answer = int.from_bytes(mystring.encode('utf-8'), \"little\")\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Pandas\n",
    "\n",
    "This time we are going to do simple \"exploratory data analysis\". To simplify, we lump all the \"iris\" data together in one big data structure called a \"pandas\" data frame (disregarding the train/valid/test split). This will allow us to use the rich libraries \"pandas\", \"numpy\", \"seaborn\", \"scikit-learn\", and others to explore the data. \n",
    "\n",
    "In the next cell, replace the \"head\" function, which just shows the first few rows of the dataset, by a pandas function providing <b>descriptive statistics</b>. To that end, you may want to check the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\">Pandas DataFrame reference page</a>. Then, in the following cell, replace the variables with their correct values and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\pankaj NIC\\GitHub\\info232\\TP0\\data/iris_feat.name_train from AutoML format\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\Users\\\\pankaj NIC\\\\GitHub\\\\info232\\\\TP0\\\\data/iris_feat.name_feat.name' does not exist: b'C:\\\\Users\\\\pankaj NIC\\\\GitHub\\\\info232\\\\TP0\\\\data/iris_feat.name_feat.name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f46d61ff7059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_as_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# With \"read_as_df\" the data are loaded as a Pandas Data Frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_as_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\GitHub\\info232\\TP0\\code\\data_io.py\u001b[0m in \u001b[0;36mread_as_df\u001b[1;34m(basename, type)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reading '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mbasename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m' from AutoML format'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mfeat_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_feat.name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mlabel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_label.name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\Users\\\\pankaj NIC\\\\GitHub\\\\info232\\\\TP0\\\\data/iris_feat.name_feat.name' does not exist: b'C:\\\\Users\\\\pankaj NIC\\\\GitHub\\\\info232\\\\TP0\\\\data/iris_feat.name_feat.name'"
     ]
    }
   ],
   "source": [
    "from data_io import read_as_df\n",
    "# With \"read_as_df\" the data are loaded as a Pandas Data Frame\n",
    "data = read_as_df(data_dir  + '/' + data_name)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 categories of flowers called \"iris\": \"setosa\", \"virginica\", and \"versicolor\". The categories are called \"target\" or \"label\": this is what we want to predict. These flowers can be characterized by the length and width of their sepals and petals. These features allows algorithms to classify correctly the flowers in their three categories. This is <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">a very famous problem introduced by Fisher</a> to illustrate the method of linear discriminant analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sepal_length = 0.1323  # Standard deviation of the sepal length, put correct value\n",
    "mean_sepal_width = 3.4454   # Mean of the sepal width, put correct value\n",
    "min_petal_length = 8.5  # Minimum value of the petal length, put correct value\n",
    "max_petal_width = 12    # Maximum value of the petal width, put correct value\n",
    "\n",
    "question = 4\n",
    "reponse = std_sepal_length+mean_sepal_width+min_petal_length+max_petal_width\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Numpy\n",
    "Pandas dataframes are very nice to explore tabular data, but sometimes, we need to perform matrix algebra and for that purpose the <a href=\"https://numpy.org/\">Numpy</a> library is often more convenient. Fortunately, it is easy to convert Pandas data structure to Numpy data structures and back.\n",
    "<br> In the cell below:\n",
    "<ul>\n",
    "    <li> Cast the data frame \"data\" into a numpy array and call it A.</li>\n",
    "    <li> Create another numpy array called B containing A without its last column (which contains non numeric values).</li>\n",
    "    <li> Create another numpy array called C containing all the values of B raised to the power 2.</li>\n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = [] # Numpy array with the values of \"data\"\n",
    "B = [] # A without the last column\n",
    "C = [] # B raised to the power 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 5\n",
    "answer = np.sum(C)\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Histograms\n",
    "\n",
    "Pandas provides you with many nice functions to explore data. Here we show you how to make histograms. The height of each bar represents the number of samples (iris flowers) in a given interval of petal (or sepal) width or length, called \"bin\". Remplace the number of bins by a smaller number (like 10) and observe the change of the bar heights. Change the variable answer to: answer=1 if the bar height increases and answer=0 otherwise. Do you understand why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(10, 10), bins=50, layout=(3, 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 6\n",
    "answer = -1           # 1 if the maximum bar height increases when the bin number decreases, 0 otherwise\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Box plots\n",
    "The <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\">Pandas DataFrame reference page</a> provides a lot of other plotting functions you may want to try. Another way of representing the density of samples for the various variables is to use <a href=\"https://en.wikipedia.org/wiki/Box_plot\">box plots</a>. Statisticians like those. The middle (green) bar represents the median of the distribution (half of the samples are above and below the median) and the span of the boxes represent the quartiles (half of the samples are within the box). The definition of the \"whiskers\" and the \"outliers\" may vary.\n",
    "<p> If we number the variables 0: sepal_length, 1: sepal_width, 2: petal_length, 3: petal_width, indicate which variable has the largest median and which one the largest quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 7\n",
    "largest_median = -1 # Variable number between 0 and 3 with largest median value\n",
    "largest_quartiles = -1 # Variable number between 0 and 3 with largest quartiles\n",
    "answer = largest_median+largest_quartiles**2    \n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Pair plots\n",
    "\n",
    "Seaborn (sns) is a package of data visualization functions: https://seaborn.pydata.org/. Quite useful! It is convenient to visualize data in 2 dimensions. One way of doing that is to plot a variable (feature) against another one, one point representing a sample (a flower). The pairplot function shows all the possibilities (off-diagonal graphs). \n",
    "\n",
    "On the diagonal, what do you see? Compare with the histograms of the previous question (for 10 bins they should be identical).\n",
    "Then add another argument to the pairplot function \n",
    "        \n",
    "        hue=\"target\" \n",
    "        \n",
    "(if you do not understand, <a href =\"https://seaborn.pydata.org/generated/seaborn.pairplot.html\">consult the DOCUMENTATION</a>). After executing the next cell again, in the following cell, answer the questions: \n",
    "\n",
    "What is the color of the iris category, which is best separated from all others?\n",
    "\n",
    "        color_best_separated = 1 if blue; 2 if orange; 3 if green.\n",
    "\n",
    "Which iris category does this correspond to?\n",
    "\n",
    "        iris_best_separated = 1 if virginica; 2 if versocolor; 3 if setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "sns.pairplot(data, diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 8\n",
    "color_best_separated = 8 # Change that\n",
    "iris_best_separated = 5  # Change that\n",
    "score += check(color_best_separated*iris_best_separated, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Feature correlation\n",
    "The variables (features) can be redundant (capture similar information). The Pearson correlation coefficient (see <a href =\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\"> Wikipedia page</a>) allows you to measure correlation (linear dependency). On a \"pair plot\", if variables are very correlated, the points are almost aligned.\n",
    "\n",
    "Looking at the \"pair plots\" above, which pair of variables looks most correlated? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 9\n",
    "# Row and column indices of the most correlated variables:\n",
    "line_number = 0              # Lines numbered de 0 to 3, remplace by the correct answer\n",
    "column_number = 0            # Columns numbered de 0 to 3, remplace by the correct answer\n",
    "score += check(line_number*column_number, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot get this right looking at the pair plots, check the correlation matrix below. Also, change the method 'pearson' to other <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\">coefficients de correlation</a>). Does this change the ranking of pairs of variables from most to least correlated? Check the definitions of <a href =\"Kandall tau\">https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient</a> and <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\">Spearman correlation coefficient</a>. Try to understand the difference between <a href=\"https://en.wikipedia.org/wiki/Correlation_and_dependence\">correlation and dependence</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_mat = data.corr(method='pearson')\n",
    "sns.heatmap(corr_mat, annot=True, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that variables can be correlated or anti-correlated. Thus maybe you want to rather use the absolute value of correlation to measure dependence. Does this change your answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(abs(corr_mat), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the matrix is symmetric and has ones on the diagonal.  To show only the values above the diagonal, see <a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\">the bottom of this post</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10: Feature selection\n",
    "Representing a matrix of coefficients with colors seems to be pretty convenient for visualization purposes. We would like to do that also for the data matrix itself. Note that, since the last column (target) contains strings (\"categorical variables\"), we first need to convert them to numbers. \n",
    "\n",
    "Observing the heatmap, which column is most correlated with the target? Insert another cell in which you plot the correlation matrix of data_new (inspiring yourself from the previous question), then confirm your intuition and anwer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "data_num = data.copy()  # If you don't use \"copy\", any change in data_num will also result in a change in data\n",
    "data_num['target']= data_num['target'].astype('category')\n",
    "data_num['target'] = data_num['target'].cat.codes\n",
    "print(data_num.head())\n",
    "sns.heatmap(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put here your code showing the correlation matrix of data_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 is here:\n",
    "What is the variable (feature) most correlated with the column \"target\"? What is the corresponding value of the Pearson correlation coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 10\n",
    "numero_variable = 0            # Variables numbered 0 to 3, replace with the correct answer\n",
    "pearson_correlation = -1       # Replace with the correct answer\n",
    "score += check(numero_variable+pearson_correlation, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Your final score is %d / 10, congratulations!' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<span style=\"color:red\">\n",
    "<br>\n",
    "    To finalize your homework:\n",
    "<b>\n",
    "<ul>\n",
    "    <li> Use  Kernel + Restart and Run all.</li>\n",
    "    <li> Save your notebook.</li>\n",
    "    <li> Push your changes to your GitHub repo with:</li>\n",
    "</ul>   \n",
    "</b>\n",
    "<pre>\n",
    "git add .\n",
    "git commit -m 'my homework is done'\n",
    "git push\n",
    "</pre>\n",
    "<br>\n",
    "</span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
